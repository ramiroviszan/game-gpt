{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1708788383630,
     "user": {
      "displayName": "Ramiro Visca",
      "userId": "02258067882625563510"
     },
     "user_tz": 180
    },
    "id": "g9pS9tYmL5Ta",
    "outputId": "8157586a-f1cc-40b6-e149-38d2c846740c"
   },
   "outputs": [],
   "source": [
    "!rm -r /workspace/content/frames_4/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23036,
     "status": "ok",
     "timestamp": 1708788409055,
     "user": {
      "displayName": "Ramiro Visca",
      "userId": "02258067882625563510"
     },
     "user_tz": 180
    },
    "id": "-jyEqfzegK77",
    "outputId": "02c2170e-9d11-4bb4-fd18-aafe2d31f3f5"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21015,
     "status": "ok",
     "timestamp": 1708788431464,
     "user": {
      "displayName": "Ramiro Visca",
      "userId": "02258067882625563510"
     },
     "user_tz": 180
    },
    "id": "4wqzTHenKPlF"
   },
   "outputs": [],
   "source": [
    "!unzip -q drive/MyDrive/Colab\\ Notebooks/GameGPT/frames_3.zip -d \"/content/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1708788432685,
     "user": {
      "displayName": "Ramiro Visca",
      "userId": "02258067882625563510"
     },
     "user_tz": 180
    },
    "id": "aO6Lsn_IyltG"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import math\n",
    "import os\n",
    "import gc\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "# Print the number of GPUs available\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "\n",
    "# Print the name of the GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = \"drive/MyDrive/Colab Notebooks/GameGPT/\"\n",
    "parent_dir = \"\"\n",
    "content_dir = f\"/content/{parent_dir}\"\n",
    "content_dir = f\"content/{parent_dir}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1708788434231,
     "user": {
      "displayName": "Ramiro Visca",
      "userId": "02258067882625563510"
     },
     "user_tz": 180
    },
    "id": "4HCiRINRsuMy"
   },
   "outputs": [],
   "source": [
    "datasetnames = ('content/frames_7', 'moves_7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OT3hMSkZrHm4"
   },
   "source": [
    "# Read Movements on each frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 552,
     "status": "ok",
     "timestamp": 1708788436238,
     "user": {
      "displayName": "Ramiro Visca",
      "userId": "02258067882625563510"
     },
     "user_tz": 180
    },
    "id": "kMWLVoTKMiTq"
   },
   "outputs": [],
   "source": [
    "moves_file_path = f'{content_dir}{datasetnames[1]}.json'\n",
    "\n",
    "# Load moves from the JSON file\n",
    "with open(moves_file_path, 'r') as json_file:\n",
    "    moves = json.load(json_file)\n",
    "value_to_int_mapping = {'Q': 0}\n",
    "unique_values = [value for value in set(moves.values()) if value != 'Q']\n",
    "value_to_int_mapping.update({value: i + 1 for i, value in enumerate(unique_values)})\n",
    "int_to_value_mapping = {v: k for k, v in value_to_int_mapping.items()}\n",
    "\n",
    "override = True\n",
    "if override:\n",
    "    value_to_int_mapping = {'Q': 0, 'L': 1, 'N': 2, 'R': 3}\n",
    "    int_to_value_mapping = {0: 'Q', 1: 'L', 2: 'N', 3: 'R'}\n",
    "    \n",
    "# Create a new dictionary with integer values\n",
    "new_moves = {key: value_to_int_mapping[value] for key, value in moves.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_to_int_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_value_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqHbdZ3brQW_"
   },
   "source": [
    "# Create Train Dataset\n",
    "\n",
    "*  frames: list of previous {seq_len} frames\n",
    "*  moves: list of input keys, one for each frame\n",
    "*  target_frames: for every {seq_len} frames there will be a target_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_folder, moves_dict, seq_len, transform=None, transform_target=None):\n",
    "        self.data_folder = data_folder\n",
    "        self.moves_dict = moves_dict\n",
    "        self.transform = transform\n",
    "        self.transform_target = transform_target\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Get a list of all image filenames in the data folder\n",
    "        self.image_filenames = [filename for filename in os.listdir(data_folder) if filename.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames) - 2  # We subtract 5 for the sequence length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get 5 consecutive frames and their moves\n",
    "        frames = [Image.open(os.path.join(self.data_folder, f'frame_{idx + i}.png')) for i in range(self.seq_len) if idx + i < len(self.image_filenames) - 2]\n",
    "        moves = [self.moves_dict[f\"{idx + i}\"] for i in range(self.seq_len) if idx + i < len(self.image_filenames) - 2]\n",
    "\n",
    "        # Apply transformations to each frame\n",
    "        if self.transform is not None:\n",
    "            frames = [self.transform(frame) for frame in frames]\n",
    "\n",
    "        # Prepare the target frames for each frame in the sequence\n",
    "        target_frames = [Image.open(os.path.join(self.data_folder, f'frame_{idx + 1 + i}.png')) for i in range(self.seq_len) if idx + 1 + i < len(self.image_filenames) - 1]\n",
    "\n",
    "        # Apply transformations to the target frame\n",
    "        if self.transform is not None:\n",
    "            target_frames = [self.transform_target(frame) for frame in target_frames]\n",
    "\n",
    "        if len(frames) == self.seq_len and len(frames) == len(moves) == len(target_frames):\n",
    "          frames = torch.stack(frames)\n",
    "          moves = torch.tensor(moves)\n",
    "          target_frames = torch.stack(target_frames)\n",
    "        else:\n",
    "          return self.__getitem__(idx - 1)\n",
    "\n",
    "        return {'frames': frames, 'moves': moves, 'target_frames': target_frames}\n",
    "\n",
    "# Define a transform for preprocessing images\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    #AddInverseBellShapeNoise(max_noise=0.7, sigma=0.3),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "data_transform_target = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiDasFkHr1Rq"
   },
   "source": [
    "# Model Definition\n",
    "Basic Image To Vector Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1708788442391,
     "user": {
      "displayName": "Ramiro Visca",
      "userId": "02258067882625563510"
     },
     "user_tz": 180
    },
    "id": "C88B4tC7NUCZ"
   },
   "outputs": [],
   "source": [
    "class ImageToVector(nn.Module):\n",
    "    def __init__(self, input_channels, image_size, conv_size, d_model, bottleneck_channels, seq_len, dropout):\n",
    "        super(ImageToVector, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.encoder1 = self.conv_block(input_channels, conv_size)\n",
    "        self.encoder2 = self.conv_block(conv_size, conv_size * 2)\n",
    "        self.encoder3 = self.conv_block(conv_size * 2, conv_size * 4)\n",
    "        self.encoder4 = self.conv_block(conv_size * 4, conv_size * 8)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.conv_block(conv_size * 8, conv_size * 16)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Fully connected layer after bottleneck\n",
    "        self.fc = nn.Linear(self._get_flattened_size(image_size), d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def _get_flattened_size(self, image_size):\n",
    "        # Calculate the size of the flattened output after all pooling layers\n",
    "        size = image_size // 2**4  # 4 pooling layers, each with stride 2\n",
    "        return (size * size * (conv_size * 16))  # channels * height * width\n",
    "         \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"Reparameterization trick for sampling from a Gaussian distribution.\"\"\"\n",
    "        std = torch.exp(0.5 * log_var)  # Standard deviation from log-variance\n",
    "        epsilon = torch.randn_like(std)  # Random noise\n",
    "        return mu + epsilon * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(F.max_pool2d(enc1, 2))\n",
    "        enc3 = self.encoder3(F.max_pool2d(enc2, 2))\n",
    "        enc4 = self.encoder4(F.max_pool2d(enc3, 2))\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(F.max_pool2d(enc4, 2))\n",
    "\n",
    "        # Flatten and pass through fully connected layer   \n",
    "        x = self.flatten(bottleneck)\n",
    "        \n",
    "         # Compute mean and log-variance\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x, bottleneck, [enc1, enc2, enc3, enc4] # Return residuals for later use in decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjDykk0Qr9yX"
   },
   "source": [
    "Input Vector Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1708788444011,
     "user": {
      "displayName": "Ramiro Visca",
      "userId": "02258067882625563510"
     },
     "user_tz": 180
    },
    "id": "I4NJCY_FgiJt"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len):  # Dynamically set d_model\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_seq_len, d_model).to(device)\n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTFiOYZgsJYP"
   },
   "source": [
    "Self Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1708788444546,
     "user": {
      "displayName": "Ramiro Visca",
      "userId": "02258067882625563510"
     },
     "user_tz": 180
    },
    "id": "SI01iVBHfLhA"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model, n_head, seq_len, dropout):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        assert d_model % n_head == 0, \"d_model must be divisible by n_head\"\n",
    "        self.c_attn = nn.Linear(d_model, 3 * d_model)\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(seq_len, seq_len)))\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        q, k, v = self.c_attn(x).split(self.d_model, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        scale = C ** -0.5\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "        attn_scores = attn_scores.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        output = self.dropout(self.linear(attn_output))  # Apply final linear transformation and dropout\n",
    "        \n",
    "        return self.norm(output + x)  # Apply residual connection and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head, seq_len, num_layers, dropout):\n",
    "        super(StackedAttention, self).__init__()\n",
    "\n",
    "        self.attention_layers = nn.ModuleList(\n",
    "            [Attention(d_model, n_head, seq_len, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn_layer in self.attention_layers:\n",
    "            x = attn_layer(x)  # Apply each attention layer sequentially\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head, seq_len_q, seq_len_k, dropout):\n",
    "        super(CrossAttention, self).__init__()\n",
    "\n",
    "        assert d_model % n_head == 0, \"d_model must be divisible by n_head\"\n",
    "        \n",
    "        self.c_attn_q = nn.Linear(d_model, d_model)\n",
    "        self.c_attn_k = nn.Linear(d_model, d_model)\n",
    "        self.c_attn_v = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Attention mask\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(seq_len_q, seq_len_k)))\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        \n",
    "    def forward(self, queries, keys, values):\n",
    "        B, T_q, C = queries.shape\n",
    "        _, T_k, _ = keys.shape\n",
    "        \n",
    "        # Apply linear projections separately\n",
    "        q = self.c_attn_q(queries)  # (B, T_q, C)\n",
    "        k = self.c_attn_k(keys)     # (B, T_k, C)\n",
    "        v = self.c_attn_v(values)   # (B, T_k, C)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(B, T_q, self.n_head, C // self.n_head).transpose(1, 2)  # (B, n_head, T_q, C // n_head)\n",
    "        k = k.view(B, T_k, self.n_head, C // self.n_head).transpose(1, 2)  # (B, n_head, T_k, C // n_head)\n",
    "        v = v.view(B, T_k, self.n_head, C // self.n_head).transpose(1, 2)  # (B, n_head, T_k, C // n_head)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scale = C ** -0.5\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * scale  # (B, n_head, T_q, T_k)\n",
    "        \n",
    "        # Apply attention mask\n",
    "        attn_scores = attn_scores.masked_fill(self.mask[:T_q, :T_k] == 0, float('-inf'))\n",
    "        \n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)  # (B, n_head, T_q, T_k)\n",
    "        attn_output = torch.matmul(attn_weights, v)  # (B, n_head, T_q, C // n_head)\n",
    "        \n",
    "        # Concatenate heads and apply final linear layer\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T_q, C)  # (B, T_q, C)\n",
    "        output = self.out_linear(attn_output)  # (B, T_q, C)\n",
    "        \n",
    "        # Apply dropout and residual connection\n",
    "        output = self.dropout(output)\n",
    "        return self.norm(output + queries)  # Residual connection and normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lx7Nx49ysNri"
   },
   "source": [
    "Basic Vector To Image decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1708788446855,
     "user": {
      "displayName": "Ramiro Visca",
      "userId": "02258067882625563510"
     },
     "user_tz": 180
    },
    "id": "25vKKTqPsu2V"
   },
   "outputs": [],
   "source": [
    "class VectorToImage(nn.Module):\n",
    "    def __init__(self, d_model, output_channels, image_size, conv_size, dropout):\n",
    "        super(VectorToImage, self).__init__()\n",
    "        self.output_channels = output_channels\n",
    "        self.image_size = image_size\n",
    "        self.conv_size = conv_size\n",
    "\n",
    "        # Calculate the size of the feature map after the fully connected layer\n",
    "        self.fc_out_size = conv_size * 16 * (image_size // 16) * (image_size // 16)\n",
    "\n",
    "        # Linear layer to expand the feature vector to the size of the desired image\n",
    "        self.fc = nn.Linear(d_model, self.fc_out_size)\n",
    "\n",
    "        self.upconv4a = self.upconv(conv_size * 16, conv_size * 8)\n",
    "        self.upconv4b = self.upconv(conv_size * 16, conv_size * 8)\n",
    "        self.decoder4 = self.conv_block(conv_size * 24, conv_size * 8)\n",
    "        \n",
    "        self.upconv3 = self.upconv(conv_size * 8, conv_size * 4)\n",
    "        self.decoder3 = self.conv_block(conv_size * 8, conv_size * 4)\n",
    "        \n",
    "        self.upconv2 = self.upconv(conv_size * 4, conv_size * 2)\n",
    "        self.decoder2 = self.conv_block(conv_size * 4, conv_size * 2)\n",
    "        \n",
    "        self.upconv1 = self.upconv(conv_size * 2, conv_size)\n",
    "        self.decoder1 = self.conv_block(conv_size * 2, conv_size)\n",
    "        \n",
    "        self.out_conv = nn.Conv2d(conv_size, output_channels, kernel_size=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def upconv(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, bottleneck, residuals):\n",
    "        # x has shape [BT, C] where BT = batch size * timesteps\n",
    "\n",
    "        enc4, enc3, enc2, enc1 = residuals[3], residuals[2], residuals[1], residuals[0]\n",
    "        # Pass through the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Reshape to match the dimensions for the first deconvolution layer\n",
    "        x = x.view(-1, self.conv_size * 16, self.image_size // 16, self.image_size // 16)\n",
    "\n",
    "        dec4a = self.upconv4a(x)\n",
    "        dec4b = self.upconv4b(bottleneck)\n",
    "        dec4 = torch.cat([dec4a, dec4b, enc4], dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        \n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat([dec3, enc3], dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        \n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat([dec2, enc2], dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        \n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat([dec1, enc1], dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        \n",
    "        out = self.out_conv(dec1)\n",
    "        \n",
    "        out= self.dropout(out)\n",
    "        out = torch.tanh(out)\n",
    " \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNoise(nn.Module):\n",
    "    def __init__(self, seq_len):\n",
    "        super(AddNoise, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def forward(self, x):\n",
    "        BT, d_model = x.size()\n",
    "        B = int(BT / self.seq_len)\n",
    "        T = self.seq_len\n",
    "        x = x.view(B, T, d_model)\n",
    "        \n",
    "        delta_noise = torch.empty(B, T, d_model, device=x.device).uniform_(-1, 1)\n",
    "        cumulative_noise = torch.zeros(B, T, d_model, device=x.device)\n",
    "        normal_samples = torch.empty(B, T, d_model, device=x.device).uniform_(0.0, 1.0)\n",
    "        mask = (normal_samples > (1 - 1 / T)).float()  # Mask where normal distribution value > 1/T       \n",
    "        delta_noise *= mask\n",
    "        for t in reversed(range(T)):\n",
    "            if t == T-1:\n",
    "                cumulative_noise[:, t] = delta_noise[:, t] \n",
    "            else:\n",
    "                cumulative_noise[:, t] = delta_noise[:, t] + cumulative_noise[:, t+1]\n",
    "\n",
    "        cumulative_noise.clip(-1, 1)\n",
    "        \n",
    "        x += cumulative_noise\n",
    "        x = x.view(-1, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0I4k9qYsXuA"
   },
   "source": [
    "Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1708788447892,
     "user": {
      "displayName": "Ramiro Visca",
      "userId": "02258067882625563510"
     },
     "user_tz": 180
    },
    "id": "LAveIsArFoke"
   },
   "outputs": [],
   "source": [
    "class GameModel(nn.Module):\n",
    "    def __init__(self, forward_type, d_model, image_size, channels, num_input_tokens, seq_len, n_head, num_a_layers, ca_n_head, conv_size, bottleneck_channels, dropout):\n",
    "        super(GameModel, self).__init__()\n",
    "\n",
    "        self.forward_type = forward_type\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.n_head = n_head\n",
    "\n",
    "        # Embedding layer for input tokens\n",
    "        self.input_token_embedding = nn.Embedding(num_input_tokens, d_model)\n",
    "        \n",
    "        # Image to vector transformation\n",
    "        self.image_to_vector = ImageToVector(input_channels=channels, image_size=image_size, conv_size=conv_size, d_model=d_model, bottleneck_channels=bottleneck_channels, seq_len=seq_len,dropout=dropout)\n",
    "       \n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len=seq_len)\n",
    "        \n",
    "        # Cross-attention layer\n",
    "        self.cross_attention = CrossAttention(d_model, ca_n_head, seq_len_q=seq_len, seq_len_k=seq_len, dropout=dropout)\n",
    "        \n",
    "        # Stacked attention layers\n",
    "        self.stacked_attention = StackedAttention(d_model, n_head, seq_len, num_a_layers, dropout)\n",
    "\n",
    "        self.add_noise = AddNoise(seq_len)\n",
    "        \n",
    "        # Output layer for image generation\n",
    "        self.vector_to_image = VectorToImage(d_model=d_model, output_channels=channels, image_size=image_size, conv_size=conv_size, dropout=dropout)\n",
    "\n",
    "        for param in self.input_token_embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "            if param.requires_grad:\n",
    "                print(f\"Updating parameter {param}\")\n",
    "        \n",
    "        for param in self.cross_attention.parameters():\n",
    "            param.requires_grad = False\n",
    "            if param.requires_grad:\n",
    "                print(f\"Updating parameter {param}\")   \n",
    "\n",
    "        if forward_type == \"autoencoder\":\n",
    "            for param in self.stacked_attention.parameters():\n",
    "                param.requires_grad = False\n",
    "                if param.requires_grad:\n",
    "                    print(f\"Updating parameter {param}\")\n",
    "\n",
    "    def forward(self, k, x):\n",
    "        if self.forward_type == \"autoencoder\":\n",
    "            return self.forward_autoencoder(k, x)\n",
    "        elif self.forward_type  == \"diffusion\":\n",
    "            return self.forward_diffusion(k,x)\n",
    "        return self.forward_full(k, x)\n",
    "    \n",
    "    def forward_autoencoder(self, k, x):\n",
    "        # Extract batch size and number of timesteps\n",
    "        B, T, _, _, _ = x.shape\n",
    "\n",
    "        image_vectors, bottleneck, residuals = self.image_to_vector(x.view(-1, *x.shape[2:]))  # Flatten batch and timesteps\n",
    "        y = image_vectors.view(B, T, -1)  # Reshape to (B, T, d_model)\n",
    "        \n",
    "        generated_images = self.vector_to_image(y.view(-1, self.d_model), residuals)  # Flatten timesteps\n",
    "        generated_images = generated_images.view(B, T, *generated_images.shape[1:])  # Reshape to (B, T, C, H, W)\n",
    "\n",
    "        return generated_images\n",
    "\n",
    "    def forward_diffusion(self, k, x):\n",
    "        # Extract batch size and number of timesteps\n",
    "        B, T, _, _, _ = x.shape\n",
    "\n",
    "        # Process all timesteps in parallel\n",
    "        # Convert images to vectors\n",
    "        image_vectors, bottleneck, residuals = self.image_to_vector(x.view(-1, *x.shape[2:]))  # Flatten batch and timesteps\n",
    "        clean_vectors = image_vectors.view(B, T, -1)  # Reshape to (B, T, d_model) \n",
    "\n",
    "        y_noised = self.add_noise(clean_vectors) \n",
    "\n",
    "        # Embed input tokens\n",
    "        k_embeddings = self.input_token_embedding(k)  # (B, T, d_model)\n",
    "        \n",
    "        # Apply cross-attention between token embeddings and self-attention output\n",
    "        y_noised = self.cross_attention(queries=y_noised, keys=k_embeddings, values=k_embeddings)  # Cross-attention\n",
    "\n",
    "        # Add positional encoding\n",
    "        y_pos = self.positional_encoding(y_noised)\n",
    "        \n",
    "        # Apply stacked self-attention\n",
    "        y_noised_pred = self.stacked_attention(y_pos)\n",
    "\n",
    "        # Add residual connection and apply layer normalization\n",
    "        #y_cleared = clean_vectors + y_noised_pred\n",
    "        \n",
    "        truth_y_noise = torch.cat((y_noised[:, 1:] + image_vectors[:, T:]), dim=1)  \n",
    "\n",
    "        return y_noised_pred, truth_noise, clean_vectors\n",
    "        \n",
    "    def forward_full(self, k, x):\n",
    "        # Extract batch size and number of timesteps\n",
    "        B, T, _, _, _ = x.shape\n",
    "        \n",
    "        # Process all timesteps in parallel\n",
    "        # Convert images to vectors\n",
    "        image_vectors, bottleneck, residuals = self.image_to_vector(x.view(-1, *x.shape[2:]))  # Flatten batch and timesteps\n",
    "        image_vectors = image_vectors.view(B, T, -1)  # Reshape to (B, T, d_model)\n",
    "        \n",
    "        # Embed input tokens\n",
    "        k_embeddings = self.input_token_embedding(k)  # (B, T, d_model)\n",
    "                \n",
    "        # Apply cross-attention between token embeddings and self-attention output\n",
    "        y = self.cross_attention(queries=image_vectors, keys=k_embeddings, values=k_embeddings)  # Cross-attention\n",
    "\n",
    "        # Add positional encoding\n",
    "        y_pos = self.positional_encoding(image_vectors)\n",
    "        \n",
    "        # Apply stacked self-attention\n",
    "        y = self.stacked_attention(y_pos)\n",
    "        \n",
    "        # Add residual connection and apply layer normalization\n",
    "        y = image_vectors + y\n",
    "        \n",
    "        # Generate images from vectors\n",
    "        generated_images = self.vector_to_image(y.view(-1, self.d_model), bottleneck, residuals)  # Flatten timesteps\n",
    "        generated_images = generated_images.view(B, T, *generated_images.shape[1:])  # Reshape to (B, T, C, H, W)\n",
    "        \n",
    "        return generated_images + x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 5\n",
    "d_model = 512\n",
    "image_size = 128\n",
    "conv_size = 64\n",
    "bottleneck_channels = 16\n",
    "n_head = 8\n",
    "num_a_layers = 4\n",
    "ca_n_head = 1\n",
    "num_input_tokens = 4\n",
    "dropout = 0.3\n",
    "channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3051,
     "status": "ok",
     "timestamp": 1708788454785,
     "user": {
      "displayName": "Ramiro Visca",
      "userId": "02258067882625563510"
     },
     "user_tz": 180
    },
    "id": "YAw7LVFA_1Q1",
    "outputId": "f30c8c2a-51f3-449e-a3b2-117dadee3d5f"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "model = GameModel(forward_type=\"full\", d_model=d_model, image_size=image_size, channels=channels, num_input_tokens=num_input_tokens, seq_len=seq_len, n_head=n_head, num_a_layers=num_a_layers, ca_n_head=ca_n_head, conv_size=conv_size, bottleneck_channels=bottleneck_channels, dropout=dropout)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1708788458370,
     "user": {
      "displayName": "Ramiro Visca",
      "userId": "02258067882625563510"
     },
     "user_tz": 180
    },
    "id": "lxjlpASvbqa1",
    "outputId": "b802fda0-a34c-4ae6-e2b9-116830af9037"
   },
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "print(f\"Memory Allocated: {torch.cuda.memory_allocated(device) / (1024**3):.2f} GB\")\n",
    "print(f\"Memory Cached: {torch.cuda.memory_reserved(device) / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0VxWm8Hsfsu"
   },
   "source": [
    "Preload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1708788460856,
     "user": {
      "displayName": "Ramiro Visca",
      "userId": "02258067882625563510"
     },
     "user_tz": 180
    },
    "id": "_3k9Uf3kvL7F"
   },
   "outputs": [],
   "source": [
    "model_path = f\"{parent_dir}model_ggptunet_v1.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13602,
     "status": "ok",
     "timestamp": 1708788477652,
     "user": {
      "displayName": "Ramiro Visca",
      "userId": "02258067882625563510"
     },
     "user_tz": 180
    },
    "id": "pirJSWT1vK2n",
    "outputId": "5312bd06-958a-46d3-fcb3-ec945e070b8d"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VelocityMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VelocityMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, predicted_frames, target_frames):\n",
    "        batch_size, seq_len, channels, height, width = predicted_frames.shape\n",
    "        \n",
    "        # Compute the MSE loss between corresponding frames\n",
    "        mse_loss = F.mse_loss(predicted_frames, target_frames, reduction='none')  # [batch_size, seq_len, channels, height, width]\n",
    "        \n",
    "        # Compute differences between consecutive frames\n",
    "        pred_diff = predicted_frames[:, 1:] - predicted_frames[:, :-1]  # [batch_size, seq_len-1, channels, height, width]\n",
    "        target_diff = target_frames[:, 1:] - target_frames[:, :-1]  # [batch_size, seq_len-1, channels, height, width]\n",
    "        \n",
    "        # Compute absolute differences to use as weights\n",
    "        target_diff_abs = torch.abs(target_diff)  # [batch_size, seq_len-1, channels, height, width]\n",
    "        \n",
    "        # Compute average weights across channels, height, and width\n",
    "        weights = target_diff_abs.mean(dim=(2, 3, 4))  # [batch_size, seq_len-1]\n",
    "        \n",
    "        # Expand weights to match the shape of mse_loss\n",
    "        weights = weights.unsqueeze(2).unsqueeze(3).unsqueeze(4)  # [batch_size, seq_len-1, 1, 1, 1]\n",
    "        weights = weights.expand(-1, -1, channels, height, width)  # [batch_size, seq_len-1, channels, height, width]\n",
    "        \n",
    "        # Adjust weights to match the original frame sequence dimensions\n",
    "        weights_full = torch.zeros_like(mse_loss)  # [batch_size, seq_len, channels, height, width]\n",
    "        weights_full[:, 1:] = weights  # Apply weights to the corresponding frame pairs\n",
    "        \n",
    "        # Apply weights and compute the weighted MSE loss\n",
    "        weighted_mse_loss = (mse_loss * weights_full).mean()  # Reduce to a single scalar\n",
    "        \n",
    "        return weighted_mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KLMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, predicted_frames, target_frames, mu, log_var):\n",
    "        batch_size, seq_len, channels, height, width = predicted_frames.shape\n",
    "        \n",
    "        # Compute the MSE loss between corresponding frames\n",
    "        mse_loss = F.mse_loss(predicted_frames, target_frames, reduction='mean')  # [batch_size, seq_len, channels, height, width]\n",
    "        \n",
    "        kl_div = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        total_loss = 0.8 * mse_loss + 0.2 * kl_div\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkvm2kvCsnz8"
   },
   "source": [
    "Train Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1708788480127,
     "user": {
      "displayName": "Ramiro Visca",
      "userId": "02258067882625563510"
     },
     "user_tz": 180
    },
    "id": "R27AuEVb1bE7"
   },
   "outputs": [],
   "source": [
    "\n",
    "velocity_criterion = VelocityMSELoss()\n",
    "mae_criterion = nn.L1Loss()  # MAE (L1 Loss)\n",
    "mse_criterion = nn.MSELoss()  # MSE (L2 Loss)\n",
    "klmse_criterion = KLMSELoss()  # MSE (L2 Loss)\n",
    "criterion = mse_criterion \n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1708788482107,
     "user": {
      "displayName": "Ramiro Visca",
      "userId": "02258067882625563510"
     },
     "user_tz": 180
    },
    "id": "IBSnP0aR_D9H"
   },
   "outputs": [],
   "source": [
    "custom_dataset = CustomDataset(datasetnames[0], new_moves, seq_len=seq_len, transform=data_transform, transform_target=data_transform_target)\n",
    "dataloader = DataLoader(custom_dataset, batch_size=2, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1708788483717,
     "user": {
      "displayName": "Ramiro Visca",
      "userId": "02258067882625563510"
     },
     "user_tz": 180
    },
    "id": "-XcDLDqNleBI",
    "outputId": "5050d9bc-bc46-43d9-f914-2f16c900ecb5"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DAOUHhptRqZ"
   },
   "source": [
    "Train Loop\n",
    "\n",
    "TODO: Val Loss Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4200128,
     "status": "ok",
     "timestamp": 1704250373937,
     "user": {
      "displayName": "Ramiro Visca",
      "userId": "11499024944947288630"
     },
     "user_tz": 180
    },
    "id": "eJXp9wLqpDcl",
    "outputId": "7670be52-b650-4d06-da26-5e41ef25247b"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "debug = False \n",
    "debug2 = False \n",
    "\n",
    "random_repeat = [1, 0]\n",
    "iters_repeat = [0, 1]\n",
    "num_epochs = len(random_repeat)\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, x in enumerate(dataloader):\n",
    "        frames = x['frames'].to(device)\n",
    "        moves = x['moves'].to(device)\n",
    "        target = x['target_frames'].to(device)\n",
    "\n",
    "        if len(frames) > 0:\n",
    "            if torch.rand(1).item() > random_repeat[epoch]:\n",
    "                with torch.no_grad():\n",
    "                    for j in range(0, iters_repeat[epoch]):\n",
    "                        first_frame = frames[:, 0:1]  # Shape [batch_size, 1, channels, height, width]\n",
    "                        predicted_frames = model(moves, frames)\n",
    "                        # Take all predicted frames except the first one (predicted for t+1 to t+seq_len)\n",
    "                        remaining_frames = predicted_frames[:, :-1]  # Shape [batch_size, seq_len-1, channels, height, width]\n",
    "                        # Concatenate the first input frame with the predicted frames\n",
    "                        frames = torch.cat((first_frame, remaining_frames), dim=1)\n",
    "                        if debug:\n",
    "                          for i in range(frames.size(1)):  # Iterating over channels\n",
    "                             predicted_frame = frames[0][i]  # Get the i-th channel frame\n",
    "                             print(predicted_frame.shape)  # Should be [128, 128]\n",
    "                        \n",
    "                             # Convert to NumPy array and adjust dimensions for PIL\n",
    "                             predicted_np = (predicted_frame.cpu().detach().numpy() * 255).astype('uint8')\n",
    "                            \n",
    "                             # Since PIL expects [height, width, channels], we need to reorder\n",
    "                             predicted_np = np.transpose(predicted_np, (1, 2, 0))  # [128, 128, 3] \n",
    "                        \n",
    "                             # Create a PIL image and display it\n",
    "                             image_pil = Image.fromarray(predicted_np)\n",
    "                             display(image_pil)  # Use this for Jupyter notebooks\n",
    "            #break\n",
    "            # Forward pass\n",
    "            output = model(moves, frames)\n",
    "\n",
    "            if debug2:\n",
    "                for i in range(output.size(1)):  # Iterating over channels\n",
    "                    predicted_frame = output[0][i]  # Get the i-th channel frame\n",
    "                    print(predicted_frame.shape)  # Should be [128, 128]\n",
    "                    \n",
    "                    # Convert to NumPy array and adjust dimensions for PIL\n",
    "                    predicted_np = (predicted_frame.cpu().detach().numpy() * 255).astype('uint8')\n",
    "                    \n",
    "                    # Since PIL expects [height, width, channels], we need to reorder\n",
    "                    predicted_np = np.transpose(predicted_np, (1, 2, 0))  # [128, 128, 3] \n",
    "                    \n",
    "                    # Create a PIL image and display it\n",
    "                    image_pil = Image.fromarray(predicted_np)\n",
    "                    display(image_pil)  # Use this for Jupyter notebooks\n",
    "                              \n",
    "            # Compute the loss\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            max_norm = 1.0  # Clip gradients to have a maximum norm of 1.0\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "            # Print the results\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "                      f\"Batch [{batch_idx + 1}/{len(dataloader)}], \"\n",
    "                      f\"Loss: {loss.item():.8f}\")\n",
    "                \n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                vloss = velocity_criterion(output, target)\n",
    "                mae = mae_criterion(output, target)\n",
    "                mse = mse_criterion(output, target)\n",
    "                # Print the results\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "                      f\"Batch [{batch_idx + 1}/{len(dataloader)}], \"\n",
    "                      f\"Loss: {loss.item():.8f}, \"\n",
    "                      f\"VLoss: {vloss.item():.8f}, \"\n",
    "                      f\"MAE: {mae.item():.8f}, \"\n",
    "                      f\"MSE: {mse.item():.8f}\")\n",
    "              \n",
    "            total_loss += loss.item()\n",
    "            #torch.cuda.empty_cache()\n",
    "            #gc.collect()\n",
    "            \n",
    "            # Save the model every 50 batches\n",
    "            if (batch_idx + 1) % 1000 == 0:\n",
    "              torch.save(model.state_dict(), f\"{parent_dir}model_checkpoint_epoch{epoch + 1}_batch{batch_idx + 1}.pth\")\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    \n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Average Loss: {average_loss:.8f}\")\n",
    "    torch.save(model.state_dict(), f\"{parent_dir}model_epoch_ended_{epoch + 1}.pth\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"{parent_dir}model_dgpt_v2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlayDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.current = Image.open(os.path.join(f\"{datasetnames[0]}/frame_0.png\"))\n",
    "        self.moves = [2, 2, 2, 2, 2]\n",
    "        self.frames = [self.current, self.current, self.current, self.current, self.current]\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1000  # We subtract 5 for the sequence length\n",
    "\n",
    "    def __getitem__(self):\n",
    "\n",
    "        frames = [data_transform_p(frame) for frame in self.frames]\n",
    "        frames = torch.stack(frames).unsqueeze(0)\n",
    "        moves = torch.tensor(self.moves).unsqueeze(0)\n",
    "\n",
    "        return {'frames': frames, 'moves': moves }\n",
    "\n",
    "    def move(self, move, frame):\n",
    "        self.moves = self.moves[1:] + [value_to_int_mapping[move]]\n",
    "        self.frames = self.frames[1:] + [frame]\n",
    "\n",
    "data_transform_p = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_dataset = PlayDataset()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "results = []\n",
    "for i in range(0, 100):\n",
    "    x = play_dataset.__getitem__()\n",
    "    \n",
    "    moves = x.get('moves').to(device)\n",
    "    frames = x.get('frames').to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(moves, frames)\n",
    "    \n",
    "    predicted_frames = output.squeeze(0).permute(0, 2, 3, 1)\n",
    "    predicted_np = (predicted_frames.cpu().detach().numpy() * 255).astype('uint8')\n",
    "    predicted = predicted_np[4]\n",
    "    image_pil = Image.fromarray(predicted)\n",
    "    movement = random.choice([\"L\", \"N\", \"R\"])\n",
    "    play_dataset.move(movement, image_pil)\n",
    "    new_size = (512, 512)  # Example new size\n",
    "    image_resized = image_pil.resize(new_size, Image.NEAREST) \n",
    "    results.append(image_resized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].save(\"play.gif\", save_all=True, append_images=results[1:], duration=0, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as ImageD\n",
    "display(ImageD(filename=\"play.gif\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "play_dataset = PlayDataset()\n",
    "model.eval()\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movements = [\"R\"] * 5 +  [\"L\"] * 10 +  [\"R\"] * 10\n",
    "for i, movement in enumerate(movements):\n",
    "    x = play_dataset.__getitem__()\n",
    "    \n",
    "    moves = x.get('moves')\n",
    "    frames = x.get('frames')\n",
    "    target = x.get('target_frames')\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(moves, frames)\n",
    "    \n",
    "    predicted_frames = output.squeeze(0).permute(0, 2, 3, 1)\n",
    "    predicted_np = (predicted_frames.cpu().detach().numpy() * 255).astype('uint8')\n",
    "    predicted = predicted_np[4]\n",
    "    image_pil = Image.fromarray(predicted)\n",
    "    play_dataset.move(movement, image_pil)\n",
    "    new_size = (512, 512)  # Example new size\n",
    "    image_resized = image_pil.resize(new_size, Image.NEAREST) \n",
    "    results.append(image_resized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].save(\"play.gif\", save_all=True, append_images=results[1:], duration=0, loop=0)\n",
    "from IPython.display import Image as ImageD\n",
    "display(ImageD(filename=\"play.gif\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play Train Moves Predicting frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlayTrainDataset(Dataset):\n",
    "    def __init__(self, data_folder, moves_dict, seq_len):\n",
    "        self.data_folder = data_folder\n",
    "        self.moves_dict = moves_dict\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Get a list of all image filenames in the data folder\n",
    "        self.image_filenames = [filename for filename in os.listdir(data_folder) if filename.endswith('.png')]\n",
    "        self.moves = [self.moves_dict[f\"{i}\"] for i in range(self.seq_len) if i < len(self.image_filenames) - 2]\n",
    "        self.frames = [Image.open(os.path.join(self.data_folder, f'frame_{i}.png')) for i in range(self.seq_len) if i < len(self.image_filenames) - 2]\n",
    "        self.counter = 4\n",
    "        \n",
    "    def __getitem__(self):\n",
    "\n",
    "        frames = [data_transform_p(frame) for frame in self.frames]\n",
    "        frames = torch.stack(frames).unsqueeze(0)\n",
    "        moves = torch.tensor(self.moves).unsqueeze(0)\n",
    "\n",
    "        return {'frames': frames, 'moves': moves }\n",
    "\n",
    "    def move(self, frame):\n",
    "        self.counter = self.counter + 1\n",
    "        self.moves = self.moves[1:] + [self.moves_dict[f\"{self.counter}\"]]\n",
    "        self.frames = self.frames[1:] + [frame]\n",
    "\n",
    "data_transform_p = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_train_dataset = PlayTrainDataset(datasetnames[0], new_moves, seq_len=5)\n",
    "model.eval()\n",
    "new_size = (512, 512)\n",
    "results = [Image.open(os.path.join(datasetnames[0], f'frame_{i}.png')).resize(new_size, Image.NEAREST)  for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 500):\n",
    "    x = play_train_dataset.__getitem__()\n",
    "    \n",
    "    moves = x.get('moves').to(device)\n",
    "    frames = x.get('frames').to(device)\n",
    "    # Forward pass\n",
    "    output = model(moves, frames)\n",
    "    \n",
    "    predicted_frames = output.squeeze(0).permute(0, 2, 3, 1)\n",
    "    predicted_np = (predicted_frames.cpu().detach().numpy() * 255).astype('uint8')\n",
    "    predicted = predicted_np[4]\n",
    "    image_pil = Image.fromarray(predicted)\n",
    "    #display(image_pil)\n",
    "    \n",
    "    play_train_dataset.move(image_pil)\n",
    "  # Example new size\n",
    "    image_resized = image_pil.resize(new_size, Image.NEAREST) \n",
    "    results.append(image_resized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].save(\"play.gif\", save_all=True, append_images=results[1:], duration=0, loop=0)\n",
    "from IPython.display import Image as ImageD\n",
    "display(ImageD(filename=\"play.gif\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict next frame only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = 500\n",
    "custom_dataset = CustomDataset(datasetnames[0], new_moves, seq_len=seq, transform=data_transform, transform_target=data_transform_target)\n",
    "dataloader = DataLoader(custom_dataset, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, x in enumerate(dataloader):\n",
    "    if batch_idx == 0:\n",
    "        frames = x['frames'].to(device)\n",
    "        moves = x['moves'].to(device)\n",
    "        target = x['target_frames'].to(device)\n",
    "     \n",
    "        if len(frames) > 0:\n",
    "            for i in range(0, frames.size(1), 5):\n",
    "                frames2 = frames[:, i:i + 5, :, :, :]\n",
    "                moves2 = moves[:, i:i + 5]\n",
    "                # Forward pass\n",
    "                output = model(moves2, frames2)\n",
    "                predicted_frames = output.squeeze(0).permute(0, 2, 3, 1)\n",
    "                predicted_np = (predicted_frames.cpu().detach().numpy() * 255).astype('uint8')\n",
    "                predicted = predicted_np[4]\n",
    "                image_pil = Image.fromarray(predicted)\n",
    "                new_size = (512, 512)  # Example new size\n",
    "                image_resized = image_pil.resize(new_size, Image.NEAREST) \n",
    "                results.append(image_resized)\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].save(\"play.gif\", save_all=True, append_images=results[1:], duration=1000, loop=0)\n",
    "from IPython.display import Image as ImageD\n",
    "display(ImageD(filename=\"play.gif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
